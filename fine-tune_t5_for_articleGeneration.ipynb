{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zJQ-5QuGVZ74"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPCyGZD8lmWLgq/WfMdJNO8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungjun-green/articleGeneratorModel/blob/main/fine-tune_t5_for_articleGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. install libraries and packages**"
      ],
      "metadata": {
        "id": "zJQ-5QuGVZ74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "grS8dhyv0by2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. load the dataset and convert it to DatasetDict format**"
      ],
      "metadata": {
        "id": "hPhQqNiMVft8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('wiki_total.json', 'r') as f:\n",
        "    data = json.load(f)\n"
      ],
      "metadata": {
        "id": "hbFWM7KkGS7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "csv_file_path = 'wiki_total.csv'\n",
        "\n",
        "# Write the CSV file\n",
        "with open(csv_file_path, mode='w', newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=['id', 'prompt', 'text'])\n",
        "    writer.writeheader()\n",
        "\n",
        "    for curr in data:\n",
        "      row = {'id': curr[\"id\"], 'prompt': curr[\"title\"], 'text': curr[\"text\"]}\n",
        "      writer.writerow(row)"
      ],
      "metadata": {
        "id": "y_XjXpO6HQ_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scrip used to combine tow csv files, if you have wiki_total.csv you can ignore this #\n",
        "\n",
        "import csv\n",
        "csv.field_size_limit(1048576) \n",
        "# Define the file paths for the two CSV files\n",
        "csv_file_path1 = 'wiki_01.csv'\n",
        "csv_file_path2 = 'wiki_02.csv'\n",
        "\n",
        "# Define the file path for the combined CSV file\n",
        "combined_csv_file_path = 'wiki_total.csv'\n",
        "\n",
        "# Read in the data from the first CSV file\n",
        "data1 = []\n",
        "with open(csv_file_path1, 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        data1.append(row)\n",
        "\n",
        "# Read in the data from the second CSV file\n",
        "data2 = []\n",
        "with open(csv_file_path2, 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        data2.append(row)\n",
        "\n",
        "# Combine the data from the two CSV files\n",
        "combined_data = data1 + data2\n",
        "\n",
        "# Write the combined data to a new CSV file\n",
        "with open(combined_csv_file_path, 'w', newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=combined_data[0].keys())\n",
        "    writer.writeheader()\n",
        "    for row in combined_data:\n",
        "        writer.writerow(row)\n"
      ],
      "metadata": {
        "id": "sQuFm2CGHmUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start from here if you imported cleaned dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the csv file into a pandas DataFrame\n",
        "df = pd.read_csv('wiki_total.csv')\n",
        "\n",
        "# Shuffle the rows\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Calculate the number of rows for each split\n",
        "num_rows = df.shape[0]\n",
        "train_rows = int(num_rows * 0.8)\n",
        "val_rows = int(num_rows * 0.1)\n",
        "\n",
        "# Split the DataFrame into training, validation, and test sets\n",
        "train_df = df.iloc[:train_rows, :]\n",
        "val_df = df.iloc[train_rows:train_rows+val_rows, :]\n",
        "test_df = df.iloc[train_rows+val_rows:, :]\n",
        "\n",
        "# Save the split data to separate csv files with the header\n",
        "train_df.to_csv('train.csv', index=False, header=True)\n",
        "val_df.to_csv('val.csv', index=False, header=True)\n",
        "test_df.to_csv('test.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "whO_0DRtSG0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        " \n",
        "# load the CSV files as Dataset \n",
        "raw_datasets = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv', 'validation': 'val.csv'})"
      ],
      "metadata": {
        "id": "j3oYlTuTSTFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_datasets)"
      ],
      "metadata": {
        "id": "u293KyTySUIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Pre-process the raw_datasets**"
      ],
      "metadata": {
        "id": "EO8IxrDZX32H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"t5-small\""
      ],
      "metadata": {
        "id": "qIyCRDLL1gYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"Write about\"\n",
        "else:\n",
        "    prefix = \"\""
      ],
      "metadata": {
        "id": "O4gNPhutV8id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 20\n",
        "max_target_length = 1024\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc + \":\" for doc in examples[\"prompt\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"text\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "NxoYAqKTV-ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "tR4tmGTqXov-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Load the model and tokenizer and set arugments**"
      ],
      "metadata": {
        "id": "-mtrD98hVnua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"t5-small\""
      ],
      "metadata": {
        "id": "Qi1KnjTyVnf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "QKl-lUGeVs7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "o4l-Al5xy3LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"textGeneration_01\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=5,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length = 1024,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "V_CK1kmRXTaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "AYH1sfGMXa47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"rouge\")"
      ],
      "metadata": {
        "id": "4XEo5LxTV7RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "cuBu3qBpXkXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "Qz8QVhLAZ9-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "tKY38Z4w87Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "oEzEGdZMZ_BI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}